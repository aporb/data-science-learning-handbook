"""
Test Orchestrator for DoD API Gateway Integration Testing

This module provides automated test execution orchestration, scheduling, and comprehensive
reporting for all types of tests in the DoD API Gateway integration suite including
unit tests, integration tests, performance tests, and security validation.

Key Features:
- Automated test suite execution and orchestration
- Parallel test execution with dependency management
- Comprehensive test reporting and analytics
- CI/CD pipeline integration
- Test result aggregation and trend analysis
- Automated failure analysis and remediation suggestions
- Performance baseline comparison and regression detection

Test Types Supported:
- Unit and Integration Tests
- Performance and Load Testing
- Security Validation and Penetration Testing
- Compliance Verification Testing
- End-to-end System Testing
"""

import asyncio
import concurrent.futures
import json
import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable, Union
from dataclasses import dataclass, asdict
from enum import Enum
import subprocess
import threading
from pathlib import Path
import shutil
import tempfile

import pytest
import yaml
from jinja2 import Template

# Import test modules
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from api_gateway.test_api_gateway import TestDoDAPIGateway, TestAPISecurityControls, TestExternalAPIClient, TestServiceMeshConfig, TestGatewayMonitoring, TestIntegration
from api_gateway.performance_benchmark import PerformanceBenchmark, LoadTestConfig, PerformanceRequirement
from api_gateway.security_validation_suite import SecurityValidationSuite, SecurityTestCategory, ComplianceStandard
from api_gateway.dod_api_gateway import DoDAGWConfig, APIGatewayEnvironment, SecurityClassification


class TestType(Enum):
    """Types of tests that can be orchestrated."""
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    SECURITY = "security"
    COMPLIANCE = "compliance"
    END_TO_END = "end_to_end"
    SMOKE = "smoke"
    REGRESSION = "regression"


class TestPriority(Enum):
    """Test execution priority levels."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class TestStatus(Enum):
    """Test execution status."""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"
    TIMEOUT = "timeout"


class ExecutionMode(Enum):
    """Test execution modes."""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    HYBRID = "hybrid"


@dataclass
class TestConfiguration:
    """Configuration for individual test execution."""
    test_id: str
    name: str
    description: str
    test_type: TestType
    priority: TestPriority
    test_class: str
    test_method: Optional[str] = None
    dependencies: List[str] = None
    timeout_seconds: int = 3600
    retry_count: int = 0
    parallel_safe: bool = True
    environment_requirements: Dict[str, Any] = None
    configuration_overrides: Dict[str, Any] = None


@dataclass
class TestExecution:
    """Test execution tracking."""
    test_id: str
    status: TestStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration_seconds: float = 0.0
    result_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    worker_id: Optional[str] = None


@dataclass
class TestSuiteConfiguration:
    """Configuration for test suite execution."""
    suite_name: str
    description: str
    execution_mode: ExecutionMode
    max_parallel_workers: int = 4
    timeout_seconds: int = 7200  # 2 hours
    failure_threshold: float = 0.1  # 10% failure threshold
    environments: List[str] = None
    include_test_types: List[TestType] = None
    exclude_test_types: List[TestType] = None
    performance_baseline: Optional[str] = None
    generate_reports: bool = True


@dataclass
class TestResult:
    """Comprehensive test result."""
    execution_id: str
    suite_name: str
    timestamp: datetime
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    error_tests: int
    execution_time_seconds: float
    success_rate: float
    test_executions: List[TestExecution]
    performance_metrics: Optional[Dict[str, Any]] = None
    security_metrics: Optional[Dict[str, Any]] = None
    compliance_metrics: Optional[Dict[str, Any]] = None
    environment_info: Dict[str, Any] = None


@dataclass
class TestReport:
    """Comprehensive test report."""
    report_id: str
    generated_at: datetime
    suite_results: List[TestResult]
    summary_statistics: Dict[str, Any]
    trend_analysis: Dict[str, Any]
    failure_analysis: Dict[str, Any]
    recommendations: List[str]
    artifacts: List[str]


class TestOrchestrator:
    """
    Comprehensive Test Orchestrator for DoD API Gateway
    
    Manages execution of all test types with proper orchestration, dependency
    management, parallel execution, and comprehensive reporting.
    """
    
    def __init__(self, gateway_config: DoDAGWConfig, 
                 redis_url: str = "redis://localhost:6379",
                 work_dir: str = None):
        """Initialize test orchestrator."""
        self.logger = logging.getLogger(__name__)
        self.gateway_config = gateway_config
        self.redis_url = redis_url
        self.work_dir = work_dir or tempfile.mkdtemp(prefix='api_gateway_tests_')
        
        # Test components
        self.performance_benchmark = None
        self.security_suite = None
        
        # Test configurations
        self.test_configurations: Dict[str, TestConfiguration] = {}
        self.suite_configurations: Dict[str, TestSuiteConfiguration] = {}
        
        # Execution tracking
        self.active_executions: Dict[str, TestExecution] = {}
        self.execution_results: List[TestResult] = []
        self.execution_history: List[TestResult] = []
        
        # Worker management
        self.worker_pool = None
        self.max_workers = 4
        
        # Initialize test configurations
        self._initialize_test_configurations()
        self._initialize_suite_configurations()
    
    async def initialize(self) -> None:
        """Initialize test orchestrator components."""
        try:
            # Initialize test components
            self.performance_benchmark = PerformanceBenchmark(self.gateway_config, self.redis_url)
            await self.performance_benchmark.initialize()
            
            self.security_suite = SecurityValidationSuite(self.gateway_config, self.redis_url)
            await self.security_suite.initialize()
            
            # Create work directories
            os.makedirs(os.path.join(self.work_dir, 'reports'), exist_ok=True)
            os.makedirs(os.path.join(self.work_dir, 'artifacts'), exist_ok=True)
            os.makedirs(os.path.join(self.work_dir, 'logs'), exist_ok=True)
            
            # Initialize worker pool
            self.worker_pool = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
            
            # Load execution history
            await self._load_execution_history()
            
            self.logger.info("Test orchestrator initialized")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize test orchestrator: {e}")
            raise
    
    def _initialize_test_configurations(self) -> None:
        """Initialize comprehensive test configurations."""
        
        # Unit Tests
        unit_tests = [
            TestConfiguration(
                test_id="UNIT-001",
                name="DoD API Gateway Unit Tests",
                description="Core API Gateway functionality tests",
                test_type=TestType.UNIT,
                priority=TestPriority.CRITICAL,
                test_class="TestDoDAPIGateway",
                timeout_seconds=300,
                parallel_safe=True
            ),
            TestConfiguration(
                test_id="UNIT-002",
                name="Security Controls Unit Tests",
                description="API security controls functionality tests",
                test_type=TestType.UNIT,
                priority=TestPriority.CRITICAL,
                test_class="TestAPISecurityControls",
                timeout_seconds=300,
                parallel_safe=True
            ),
            TestConfiguration(
                test_id="UNIT-003",
                name="External API Client Unit Tests",
                description="External API client functionality tests",
                test_type=TestType.UNIT,
                priority=TestPriority.HIGH,
                test_class="TestExternalAPIClient",
                timeout_seconds=300,
                parallel_safe=True
            ),
            TestConfiguration(
                test_id="UNIT-004",
                name="Service Mesh Config Unit Tests",
                description="Service mesh configuration tests",
                test_type=TestType.UNIT,
                priority=TestPriority.MEDIUM,
                test_class="TestServiceMeshConfig",
                timeout_seconds=300,
                parallel_safe=True
            ),
            TestConfiguration(
                test_id="UNIT-005",
                name="Gateway Monitoring Unit Tests",
                description="Gateway monitoring functionality tests",
                test_type=TestType.UNIT,
                priority=TestPriority.HIGH,
                test_class="TestGatewayMonitoring",
                timeout_seconds=300,
                parallel_safe=True
            )
        ]\n        \n        # Integration Tests\n        integration_tests = [\n            TestConfiguration(\n                test_id=\"INTEGRATION-001\",\n                name=\"End-to-End Request Flow\",\n                description=\"Complete request flow through API Gateway\",\n                test_type=TestType.INTEGRATION,\n                priority=TestPriority.CRITICAL,\n                test_class=\"TestIntegration\",\n                test_method=\"test_end_to_end_request_flow\",\n                dependencies=[\"UNIT-001\", \"UNIT-002\"],\n                timeout_seconds=600,\n                parallel_safe=False\n            ),\n            TestConfiguration(\n                test_id=\"INTEGRATION-002\",\n                name=\"Security Incident Response\",\n                description=\"Security incident detection and response workflow\",\n                test_type=TestType.INTEGRATION,\n                priority=TestPriority.CRITICAL,\n                test_class=\"TestIntegration\",\n                test_method=\"test_security_incident_response\",\n                dependencies=[\"UNIT-002\"],\n                timeout_seconds=600,\n                parallel_safe=False\n            ),\n            TestConfiguration(\n                test_id=\"INTEGRATION-003\",\n                name=\"High Availability Scenarios\",\n                description=\"High availability and failover scenarios\",\n                test_type=TestType.INTEGRATION,\n                priority=TestPriority.HIGH,\n                test_class=\"TestIntegration\",\n                test_method=\"test_high_availability_scenarios\",\n                dependencies=[\"UNIT-003\"],\n                timeout_seconds=900,\n                parallel_safe=False\n            ),\n            TestConfiguration(\n                test_id=\"INTEGRATION-004\",\n                name=\"Classification Data Handling\",\n                description=\"Proper handling of classified data\",\n                test_type=TestType.INTEGRATION,\n                priority=TestPriority.CRITICAL,\n                test_class=\"TestIntegration\",\n                test_method=\"test_classification_data_handling\",\n                dependencies=[\"UNIT-001\", \"UNIT-002\"],\n                timeout_seconds=600,\n                parallel_safe=False\n            )\n        ]\n        \n        # Performance Tests\n        performance_tests = [\n            TestConfiguration(\n                test_id=\"PERF-001\",\n                name=\"Standard Load Test\",\n                description=\"Standard load test with gradual ramp-up\",\n                test_type=TestType.PERFORMANCE,\n                priority=TestPriority.HIGH,\n                test_class=\"PerformanceBenchmark\",\n                test_method=\"run_load_test\",\n                dependencies=[\"INTEGRATION-001\"],\n                timeout_seconds=1800,\n                parallel_safe=False,\n                configuration_overrides={\"test_config\": \"standard_load_test\"}\n            ),\n            TestConfiguration(\n                test_id=\"PERF-002\",\n                name=\"Stress Test\",\n                description=\"Stress test to find system breaking point\",\n                test_type=TestType.PERFORMANCE,\n                priority=TestPriority.MEDIUM,\n                test_class=\"PerformanceBenchmark\",\n                test_method=\"run_load_test\",\n                dependencies=[\"PERF-001\"],\n                timeout_seconds=2400,\n                parallel_safe=False,\n                configuration_overrides={\"test_config\": \"stress_test\"}\n            ),\n            TestConfiguration(\n                test_id=\"PERF-003\",\n                name=\"Spike Test\",\n                description=\"Spike test with sudden load increase\",\n                test_type=TestType.PERFORMANCE,\n                priority=TestPriority.MEDIUM,\n                test_class=\"PerformanceBenchmark\",\n                test_method=\"run_load_test\",\n                dependencies=[\"INTEGRATION-001\"],\n                timeout_seconds=900,\n                parallel_safe=False,\n                configuration_overrides={\"test_config\": \"spike_test\"}\n            )\n        ]\n        \n        # Security Tests\n        security_tests = [\n            TestConfiguration(\n                test_id=\"SEC-001\",\n                name=\"Authentication Security Tests\",\n                description=\"Authentication mechanism security validation\",\n                test_type=TestType.SECURITY,\n                priority=TestPriority.CRITICAL,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"UNIT-002\"],\n                timeout_seconds=1200,\n                parallel_safe=True,\n                configuration_overrides={\"categories\": [\"AUTHENTICATION\"]}\n            ),\n            TestConfiguration(\n                test_id=\"SEC-002\",\n                name=\"Input Validation Security Tests\",\n                description=\"Input validation and injection attack protection\",\n                test_type=TestType.SECURITY,\n                priority=TestPriority.CRITICAL,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"UNIT-002\"],\n                timeout_seconds=1200,\n                parallel_safe=True,\n                configuration_overrides={\"categories\": [\"INPUT_VALIDATION\"]}\n            ),\n            TestConfiguration(\n                test_id=\"SEC-003\",\n                name=\"Cryptography Security Tests\",\n                description=\"Cryptographic implementation validation\",\n                test_type=TestType.SECURITY,\n                priority=TestPriority.HIGH,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"UNIT-001\"],\n                timeout_seconds=900,\n                parallel_safe=True,\n                configuration_overrides={\"categories\": [\"CRYPTOGRAPHY\"]}\n            ),\n            TestConfiguration(\n                test_id=\"SEC-004\",\n                name=\"Penetration Tests\",\n                description=\"Automated penetration testing\",\n                test_type=TestType.SECURITY,\n                priority=TestPriority.HIGH,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_penetration_tests\",\n                dependencies=[\"SEC-001\", \"SEC-002\"],\n                timeout_seconds=1800,\n                parallel_safe=False\n            )\n        ]\n        \n        # Compliance Tests\n        compliance_tests = [\n            TestConfiguration(\n                test_id=\"COMP-001\",\n                name=\"DoD 8500 Series Compliance\",\n                description=\"DoD 8500 series compliance validation\",\n                test_type=TestType.COMPLIANCE,\n                priority=TestPriority.CRITICAL,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"SEC-001\", \"SEC-002\", \"SEC-003\"],\n                timeout_seconds=1800,\n                parallel_safe=True,\n                configuration_overrides={\"compliance_standard\": \"DOD_8500_SERIES\"}\n            ),\n            TestConfiguration(\n                test_id=\"COMP-002\",\n                name=\"NIST 800-53 Compliance\",\n                description=\"NIST 800-53 security controls compliance\",\n                test_type=TestType.COMPLIANCE,\n                priority=TestPriority.CRITICAL,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"SEC-001\", \"SEC-002\", \"SEC-003\"],\n                timeout_seconds=1800,\n                parallel_safe=True,\n                configuration_overrides={\"compliance_standard\": \"NIST_800_53\"}\n            ),\n            TestConfiguration(\n                test_id=\"COMP-003\",\n                name=\"STIG Compliance\",\n                description=\"Security Technical Implementation Guide compliance\",\n                test_type=TestType.COMPLIANCE,\n                priority=TestPriority.HIGH,\n                test_class=\"SecurityValidationSuite\",\n                test_method=\"run_security_test_suite\",\n                dependencies=[\"SEC-003\"],\n                timeout_seconds=1200,\n                parallel_safe=True,\n                configuration_overrides={\"compliance_standard\": \"STIG\"}\n            )\n        ]\n        \n        # Add all tests to configurations\n        all_tests = unit_tests + integration_tests + performance_tests + security_tests + compliance_tests\n        for test_config in all_tests:\n            self.test_configurations[test_config.test_id] = test_config\n    \n    def _initialize_suite_configurations(self) -> None:\n        \"\"\"Initialize test suite configurations.\"\"\"\n        \n        # Smoke Test Suite - Quick validation\n        self.suite_configurations[\"smoke\"] = TestSuiteConfiguration(\n            suite_name=\"Smoke Test Suite\",\n            description=\"Quick smoke tests for basic functionality validation\",\n            execution_mode=ExecutionMode.PARALLEL,\n            max_parallel_workers=4,\n            timeout_seconds=1800,\n            include_test_types=[TestType.UNIT],\n            generate_reports=True\n        )\n        \n        # Full Regression Suite - Comprehensive testing\n        self.suite_configurations[\"regression\"] = TestSuiteConfiguration(\n            suite_name=\"Full Regression Test Suite\",\n            description=\"Comprehensive regression testing suite\",\n            execution_mode=ExecutionMode.HYBRID,\n            max_parallel_workers=4,\n            timeout_seconds=14400,  # 4 hours\n            failure_threshold=0.05,  # 5% failure threshold\n            generate_reports=True\n        )\n        \n        # Security Validation Suite\n        self.suite_configurations[\"security\"] = TestSuiteConfiguration(\n            suite_name=\"Security Validation Suite\",\n            description=\"Comprehensive security testing and validation\",\n            execution_mode=ExecutionMode.PARALLEL,\n            max_parallel_workers=3,\n            timeout_seconds=7200,\n            include_test_types=[TestType.SECURITY, TestType.COMPLIANCE],\n            generate_reports=True\n        )\n        \n        # Performance Testing Suite\n        self.suite_configurations[\"performance\"] = TestSuiteConfiguration(\n            suite_name=\"Performance Testing Suite\",\n            description=\"Performance and load testing validation\",\n            execution_mode=ExecutionMode.SEQUENTIAL,\n            max_parallel_workers=1,\n            timeout_seconds=10800,  # 3 hours\n            include_test_types=[TestType.PERFORMANCE],\n            generate_reports=True\n        )\n        \n        # CI/CD Pipeline Suite - Fast feedback\n        self.suite_configurations[\"ci_cd\"] = TestSuiteConfiguration(\n            suite_name=\"CI/CD Pipeline Suite\",\n            description=\"Fast feedback tests for CI/CD pipeline\",\n            execution_mode=ExecutionMode.PARALLEL,\n            max_parallel_workers=6,\n            timeout_seconds=3600,\n            failure_threshold=0.02,  # 2% failure threshold\n            include_test_types=[TestType.UNIT, TestType.INTEGRATION],\n            exclude_test_types=[TestType.PERFORMANCE],\n            generate_reports=True\n        )\n    \n    async def execute_test_suite(self, suite_name: str, \n                               environment_overrides: Optional[Dict[str, Any]] = None) -> TestResult:\n        \"\"\"Execute a configured test suite.\"\"\"\n        if suite_name not in self.suite_configurations:\n            raise ValueError(f\"Unknown test suite: {suite_name}\")\n        \n        suite_config = self.suite_configurations[suite_name]\n        execution_id = str(uuid.uuid4())\n        start_time = datetime.utcnow()\n        \n        self.logger.info(f\"Starting test suite execution: {suite_name} (ID: {execution_id})\")\n        \n        try:\n            # Filter tests based on suite configuration\n            tests_to_execute = self._filter_tests_for_suite(suite_config)\n            \n            # Apply environment overrides\n            if environment_overrides:\n                for test_id in tests_to_execute:\n                    if self.test_configurations[test_id].environment_requirements is None:\n                        self.test_configurations[test_id].environment_requirements = {}\n                    self.test_configurations[test_id].environment_requirements.update(environment_overrides)\n            \n            # Execute tests based on execution mode\n            if suite_config.execution_mode == ExecutionMode.SEQUENTIAL:\n                test_executions = await self._execute_tests_sequential(tests_to_execute, suite_config)\n            elif suite_config.execution_mode == ExecutionMode.PARALLEL:\n                test_executions = await self._execute_tests_parallel(tests_to_execute, suite_config)\n            else:  # HYBRID\n                test_executions = await self._execute_tests_hybrid(tests_to_execute, suite_config)\n            \n            # Calculate results\n            end_time = datetime.utcnow()\n            execution_time = (end_time - start_time).total_seconds()\n            \n            total_tests = len(test_executions)\n            passed_tests = len([e for e in test_executions if e.status == TestStatus.PASSED])\n            failed_tests = len([e for e in test_executions if e.status == TestStatus.FAILED])\n            skipped_tests = len([e for e in test_executions if e.status == TestStatus.SKIPPED])\n            error_tests = len([e for e in test_executions if e.status == TestStatus.ERROR])\n            \n            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n            \n            # Collect performance and security metrics\n            performance_metrics = await self._collect_performance_metrics(test_executions)\n            security_metrics = await self._collect_security_metrics(test_executions)\n            compliance_metrics = await self._collect_compliance_metrics(test_executions)\n            \n            # Create test result\n            result = TestResult(\n                execution_id=execution_id,\n                suite_name=suite_name,\n                timestamp=start_time,\n                total_tests=total_tests,\n                passed_tests=passed_tests,\n                failed_tests=failed_tests,\n                skipped_tests=skipped_tests,\n                error_tests=error_tests,\n                execution_time_seconds=execution_time,\n                success_rate=success_rate,\n                test_executions=test_executions,\n                performance_metrics=performance_metrics,\n                security_metrics=security_metrics,\n                compliance_metrics=compliance_metrics,\n                environment_info=self._collect_environment_info()\n            )\n            \n            self.execution_results.append(result)\n            await self._save_execution_result(result)\n            \n            # Generate reports if configured\n            if suite_config.generate_reports:\n                await self._generate_test_reports(result)\n            \n            self.logger.info(f\"Test suite completed: {suite_name} - {passed_tests}/{total_tests} passed ({success_rate:.1f}%)\")\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Test suite execution failed: {e}\")\n            raise\n    \n    def _filter_tests_for_suite(self, suite_config: TestSuiteConfiguration) -> List[str]:\n        \"\"\"Filter tests based on suite configuration.\"\"\"\n        filtered_tests = []\n        \n        for test_id, test_config in self.test_configurations.items():\n            # Include/exclude by test type\n            if suite_config.include_test_types and test_config.test_type not in suite_config.include_test_types:\n                continue\n            \n            if suite_config.exclude_test_types and test_config.test_type in suite_config.exclude_test_types:\n                continue\n            \n            filtered_tests.append(test_id)\n        \n        return filtered_tests\n    \n    async def _execute_tests_sequential(self, test_ids: List[str], \n                                       suite_config: TestSuiteConfiguration) -> List[TestExecution]:\n        \"\"\"Execute tests sequentially with dependency resolution.\"\"\"\n        executions = []\n        completed_tests = set()\n        \n        # Sort tests by dependencies\n        sorted_tests = self._sort_tests_by_dependencies(test_ids)\n        \n        for test_id in sorted_tests:\n            # Check if dependencies are satisfied\n            test_config = self.test_configurations[test_id]\n            if test_config.dependencies:\n                missing_deps = set(test_config.dependencies) - completed_tests\n                if missing_deps:\n                    self.logger.warning(f\"Skipping test {test_id} due to missing dependencies: {missing_deps}\")\n                    execution = TestExecution(\n                        test_id=test_id,\n                        status=TestStatus.SKIPPED,\n                        error_message=f\"Missing dependencies: {missing_deps}\"\n                    )\n                    executions.append(execution)\n                    continue\n            \n            # Execute test\n            execution = await self._execute_single_test(test_id)\n            executions.append(execution)\n            \n            if execution.status == TestStatus.PASSED:\n                completed_tests.add(test_id)\n            \n            # Check failure threshold\n            failed_count = len([e for e in executions if e.status in [TestStatus.FAILED, TestStatus.ERROR]])\n            if failed_count / len(executions) > suite_config.failure_threshold:\n                self.logger.warning(f\"Failure threshold exceeded ({suite_config.failure_threshold * 100}%), stopping execution\")\n                # Mark remaining tests as skipped\n                remaining_tests = sorted_tests[sorted_tests.index(test_id) + 1:]\n                for remaining_test in remaining_tests:\n                    execution = TestExecution(\n                        test_id=remaining_test,\n                        status=TestStatus.SKIPPED,\n                        error_message=\"Skipped due to failure threshold exceeded\"\n                    )\n                    executions.append(execution)\n                break\n        \n        return executions\n    \n    async def _execute_tests_parallel(self, test_ids: List[str], \n                                     suite_config: TestSuiteConfiguration) -> List[TestExecution]:\n        \"\"\"Execute tests in parallel with dependency management.\"\"\"\n        executions = []\n        completed_tests = set()\n        pending_tests = set(test_ids)\n        running_tasks = {}\n        \n        while pending_tests or running_tasks:\n            # Start new tests that have satisfied dependencies\n            ready_tests = []\n            for test_id in list(pending_tests):\n                test_config = self.test_configurations[test_id]\n                if not test_config.dependencies or set(test_config.dependencies).issubset(completed_tests):\n                    if test_config.parallel_safe and len(running_tasks) < suite_config.max_parallel_workers:\n                        ready_tests.append(test_id)\n            \n            # Start ready tests\n            for test_id in ready_tests:\n                if len(running_tasks) >= suite_config.max_parallel_workers:\n                    break\n                \n                task = asyncio.create_task(self._execute_single_test(test_id))\n                running_tasks[test_id] = task\n                pending_tests.remove(test_id)\n            \n            # Wait for any test to complete\n            if running_tasks:\n                done, _ = await asyncio.wait(running_tasks.values(), return_when=asyncio.FIRST_COMPLETED)\n                \n                for task in done:\n                    # Find which test completed\n                    completed_test_id = None\n                    for test_id, test_task in running_tasks.items():\n                        if test_task == task:\n                            completed_test_id = test_id\n                            break\n                    \n                    if completed_test_id:\n                        execution = await task\n                        executions.append(execution)\n                        \n                        if execution.status == TestStatus.PASSED:\n                            completed_tests.add(completed_test_id)\n                        \n                        del running_tasks[completed_test_id]\n                        \n                        # Check failure threshold\n                        failed_count = len([e for e in executions if e.status in [TestStatus.FAILED, TestStatus.ERROR]])\n                        if failed_count / len(executions) > suite_config.failure_threshold:\n                            self.logger.warning(f\"Failure threshold exceeded, cancelling remaining tests\")\n                            # Cancel running tasks\n                            for remaining_task in running_tasks.values():\n                                remaining_task.cancel()\n                            # Mark pending tests as skipped\n                            for pending_test in pending_tests:\n                                execution = TestExecution(\n                                    test_id=pending_test,\n                                    status=TestStatus.SKIPPED,\n                                    error_message=\"Skipped due to failure threshold exceeded\"\n                                )\n                                executions.append(execution)\n                            return executions\n            \n            # Small delay to prevent busy waiting\n            await asyncio.sleep(0.1)\n        \n        return executions\n    \n    async def _execute_tests_hybrid(self, test_ids: List[str], \n                                   suite_config: TestSuiteConfiguration) -> List[TestExecution]:\n        \"\"\"Execute tests using hybrid approach (parallel where safe, sequential where needed).\"\"\"\n        # Separate parallel-safe and sequential-only tests\n        parallel_tests = [tid for tid in test_ids if self.test_configurations[tid].parallel_safe]\n        sequential_tests = [tid for tid in test_ids if not self.test_configurations[tid].parallel_safe]\n        \n        executions = []\n        \n        # Execute parallel-safe tests first\n        if parallel_tests:\n            parallel_executions = await self._execute_tests_parallel(parallel_tests, suite_config)\n            executions.extend(parallel_executions)\n        \n        # Execute sequential tests\n        if sequential_tests:\n            sequential_executions = await self._execute_tests_sequential(sequential_tests, suite_config)\n            executions.extend(sequential_executions)\n        \n        return executions\n    \n    async def _execute_single_test(self, test_id: str) -> TestExecution:\n        \"\"\"Execute a single test with proper error handling and timeouts.\"\"\"\n        test_config = self.test_configurations[test_id]\n        execution = TestExecution(\n            test_id=test_id,\n            status=TestStatus.RUNNING,\n            start_time=datetime.utcnow(),\n            worker_id=threading.current_thread().name\n        )\n        \n        self.active_executions[test_id] = execution\n        \n        try:\n            self.logger.info(f\"Starting test: {test_id} - {test_config.name}\")\n            \n            # Execute test based on type and configuration\n            result_data = None\n            \n            if test_config.test_class == \"TestDoDAPIGateway\":\n                result_data = await self._execute_pytest_test(test_config)\n            elif test_config.test_class == \"PerformanceBenchmark\":\n                result_data = await self._execute_performance_test(test_config)\n            elif test_config.test_class == \"SecurityValidationSuite\":\n                result_data = await self._execute_security_test(test_config)\n            else:\n                result_data = await self._execute_pytest_test(test_config)\n            \n            # Determine test status from result\n            if result_data and result_data.get('success', False):\n                execution.status = TestStatus.PASSED\n            else:\n                execution.status = TestStatus.FAILED\n                execution.error_message = result_data.get('error', 'Test failed') if result_data else 'No result data'\n            \n            execution.result_data = result_data\n            \n        except asyncio.TimeoutError:\n            execution.status = TestStatus.TIMEOUT\n            execution.error_message = f\"Test timed out after {test_config.timeout_seconds} seconds\"\n            \n        except Exception as e:\n            execution.status = TestStatus.ERROR\n            execution.error_message = str(e)\n            self.logger.error(f\"Test {test_id} failed with error: {e}\")\n        \n        finally:\n            execution.end_time = datetime.utcnow()\n            if execution.start_time:\n                execution.duration_seconds = (execution.end_time - execution.start_time).total_seconds()\n            \n            if test_id in self.active_executions:\n                del self.active_executions[test_id]\n            \n            self.logger.info(f\"Test completed: {test_id} - {execution.status.value.upper()} ({execution.duration_seconds:.2f}s)\")\n        \n        return execution\n    \n    async def _execute_pytest_test(self, test_config: TestConfiguration) -> Dict[str, Any]:\n        \"\"\"Execute pytest-based test.\"\"\"\n        try:\n            # Construct pytest command\n            test_file = \"test_api_gateway.py\"\n            test_path = os.path.join(os.path.dirname(__file__), test_file)\n            \n            if test_config.test_method:\n                test_target = f\"{test_path}::{test_config.test_class}::{test_config.test_method}\"\n            else:\n                test_target = f\"{test_path}::{test_config.test_class}\"\n            \n            # Run pytest with timeout\n            cmd = [\"python\", \"-m\", \"pytest\", test_target, \"-v\", \"--tb=short\"]\n            \n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd=os.path.dirname(__file__)\n            )\n            \n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    process.communicate(),\n                    timeout=test_config.timeout_seconds\n                )\n                \n                success = process.returncode == 0\n                \n                return {\n                    'success': success,\n                    'returncode': process.returncode,\n                    'stdout': stdout.decode() if stdout else '',\n                    'stderr': stderr.decode() if stderr else '',\n                    'error': stderr.decode() if stderr and not success else None\n                }\n                \n            except asyncio.TimeoutError:\n                process.kill()\n                await process.wait()\n                raise asyncio.TimeoutError()\n                \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def _execute_performance_test(self, test_config: TestConfiguration) -> Dict[str, Any]:\n        \"\"\"Execute performance test.\"\"\"\n        try:\n            # Get test configuration\n            config_name = test_config.configuration_overrides.get('test_config', 'standard_load_test')\n            \n            # Import and create test config\n            from api_gateway.performance_benchmark import (\n                create_standard_load_test, create_stress_test, create_spike_test\n            )\n            \n            config_map = {\n                'standard_load_test': create_standard_load_test,\n                'stress_test': create_stress_test,\n                'spike_test': create_spike_test\n            }\n            \n            if config_name not in config_map:\n                raise ValueError(f\"Unknown performance test config: {config_name}\")\n            \n            load_config = config_map[config_name]()\n            \n            # Execute performance test\n            result = await self.performance_benchmark.run_load_test(load_config)\n            \n            return {\n                'success': result.requirements_met,\n                'performance_result': asdict(result),\n                'error': None if result.requirements_met else f\"Performance requirements not met: {result.failed_requirements}\"\n            }\n            \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def _execute_security_test(self, test_config: TestConfiguration) -> Dict[str, Any]:\n        \"\"\"Execute security test.\"\"\"\n        try:\n            # Get test configuration\n            categories = test_config.configuration_overrides.get('categories')\n            compliance_standard = test_config.configuration_overrides.get('compliance_standard')\n            \n            if test_config.test_method == 'run_security_test_suite':\n                # Convert category strings to enums if provided\n                test_categories = None\n                if categories:\n                    from api_gateway.security_validation_suite import SecurityTestCategory\n                    test_categories = [SecurityTestCategory(cat) for cat in categories]\n                \n                result = await self.security_suite.run_security_test_suite(test_categories)\n                \n                return {\n                    'success': result.get('critical_failures', 0) == 0,\n                    'security_result': result,\n                    'error': None if result.get('critical_failures', 0) == 0 else f\"Critical security failures found: {result.get('critical_failures', 0)}\"\n                }\n                \n            elif test_config.test_method == 'run_penetration_tests':\n                results = await self.security_suite.run_penetration_tests()\n                \n                # Check if any critical vulnerabilities were found\n                critical_vulns = [r for r in results if r.vulnerability_found and r.risk_level.value in ['critical', 'high']]\n                \n                return {\n                    'success': len(critical_vulns) == 0,\n                    'penetration_results': [asdict(r) for r in results],\n                    'error': None if len(critical_vulns) == 0 else f\"Critical vulnerabilities found: {len(critical_vulns)}\"\n                }\n            \n            else:\n                raise ValueError(f\"Unknown security test method: {test_config.test_method}\")\n                \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def _sort_tests_by_dependencies(self, test_ids: List[str]) -> List[str]:\n        \"\"\"Sort tests by dependencies using topological sort.\"\"\"\n        # Simple topological sort implementation\n        sorted_tests = []\n        remaining_tests = set(test_ids)\n        \n        while remaining_tests:\n            # Find tests with no unresolved dependencies\n            ready_tests = []\n            for test_id in remaining_tests:\n                test_config = self.test_configurations[test_id]\n                if not test_config.dependencies or all(dep in sorted_tests for dep in test_config.dependencies):\n                    ready_tests.append(test_id)\n            \n            if not ready_tests:\n                # Circular dependency or missing dependency, add remaining tests anyway\n                ready_tests = list(remaining_tests)\n            \n            # Add ready tests to sorted list\n            for test_id in ready_tests:\n                sorted_tests.append(test_id)\n                remaining_tests.remove(test_id)\n        \n        return sorted_tests\n    \n    async def _collect_performance_metrics(self, executions: List[TestExecution]) -> Optional[Dict[str, Any]]:\n        \"\"\"Collect performance metrics from test executions.\"\"\"\n        performance_executions = [\n            e for e in executions \n            if self.test_configurations[e.test_id].test_type == TestType.PERFORMANCE and \n            e.result_data and 'performance_result' in e.result_data\n        ]\n        \n        if not performance_executions:\n            return None\n        \n        # Aggregate performance metrics\n        total_requests = sum(\n            e.result_data['performance_result']['total_requests'] \n            for e in performance_executions\n        )\n        \n        avg_response_time = sum(\n            e.result_data['performance_result']['avg_response_time'] \n            for e in performance_executions\n        ) / len(performance_executions)\n        \n        avg_throughput = sum(\n            e.result_data['performance_result']['requests_per_second'] \n            for e in performance_executions\n        ) / len(performance_executions)\n        \n        return {\n            'total_requests': total_requests,\n            'average_response_time': avg_response_time,\n            'average_throughput': avg_throughput,\n            'performance_tests_count': len(performance_executions)\n        }\n    \n    async def _collect_security_metrics(self, executions: List[TestExecution]) -> Optional[Dict[str, Any]]:\n        \"\"\"Collect security metrics from test executions.\"\"\"\n        security_executions = [\n            e for e in executions \n            if self.test_configurations[e.test_id].test_type == TestType.SECURITY and \n            e.result_data\n        ]\n        \n        if not security_executions:\n            return None\n        \n        # Aggregate security metrics\n        total_security_tests = sum(\n            e.result_data.get('security_result', {}).get('total_tests', 0)\n            for e in security_executions\n            if 'security_result' in e.result_data\n        )\n        \n        total_passed_tests = sum(\n            e.result_data.get('security_result', {}).get('passed_tests', 0)\n            for e in security_executions\n            if 'security_result' in e.result_data\n        )\n        \n        critical_failures = sum(\n            e.result_data.get('security_result', {}).get('critical_failures', 0)\n            for e in security_executions\n            if 'security_result' in e.result_data\n        )\n        \n        return {\n            'total_security_tests': total_security_tests,\n            'passed_security_tests': total_passed_tests,\n            'critical_security_failures': critical_failures,\n            'security_test_suites': len(security_executions)\n        }\n    \n    async def _collect_compliance_metrics(self, executions: List[TestExecution]) -> Optional[Dict[str, Any]]:\n        \"\"\"Collect compliance metrics from test executions.\"\"\"\n        compliance_executions = [\n            e for e in executions \n            if self.test_configurations[e.test_id].test_type == TestType.COMPLIANCE and \n            e.result_data\n        ]\n        \n        if not compliance_executions:\n            return None\n        \n        # Aggregate compliance metrics\n        compliance_scores = []\n        for execution in compliance_executions:\n            if 'security_result' in execution.result_data:\n                compliance_summary = execution.result_data['security_result'].get('compliance_summary', {})\n                for standard, score in compliance_summary.items():\n                    compliance_scores.append({'standard': standard, 'score': score})\n        \n        return {\n            'compliance_tests_count': len(compliance_executions),\n            'compliance_scores': compliance_scores\n        }\n    \n    def _collect_environment_info(self) -> Dict[str, Any]:\n        \"\"\"Collect environment information.\"\"\"\n        import platform\n        import psutil\n        \n        return {\n            'platform': platform.platform(),\n            'python_version': platform.python_version(),\n            'cpu_count': psutil.cpu_count(),\n            'memory_total_gb': psutil.virtual_memory().total / (1024**3),\n            'timestamp': datetime.utcnow().isoformat(),\n            'gateway_config': {\n                'environment': self.gateway_config.environment.value,\n                'service_name': self.gateway_config.service_name,\n                'classification': self.gateway_config.security_classification.value\n            }\n        }\n    \n    async def _save_execution_result(self, result: TestResult) -> None:\n        \"\"\"Save test execution result to persistent storage.\"\"\"\n        try:\n            result_file = os.path.join(self.work_dir, 'results', f'test_result_{result.execution_id}.json')\n            os.makedirs(os.path.dirname(result_file), exist_ok=True)\n            \n            with open(result_file, 'w') as f:\n                json.dump(asdict(result), f, indent=2, default=str)\n            \n            # Also save to history\n            history_file = os.path.join(self.work_dir, 'execution_history.json')\n            if os.path.exists(history_file):\n                with open(history_file, 'r') as f:\n                    history = json.load(f)\n            else:\n                history = []\n            \n            history.append(asdict(result))\n            \n            with open(history_file, 'w') as f:\n                json.dump(history, f, indent=2, default=str)\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to save execution result: {e}\")\n    \n    async def _load_execution_history(self) -> None:\n        \"\"\"Load execution history from persistent storage.\"\"\"\n        try:\n            history_file = os.path.join(self.work_dir, 'execution_history.json')\n            if os.path.exists(history_file):\n                with open(history_file, 'r') as f:\n                    history_data = json.load(f)\n                \n                for result_data in history_data:\n                    # Convert back to TestResult object\n                    result = TestResult(**result_data)\n                    self.execution_history.append(result)\n                \n                self.logger.info(f\"Loaded {len(self.execution_history)} historical test results\")\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to load execution history: {e}\")\n    \n    async def _generate_test_reports(self, result: TestResult) -> None:\n        \"\"\"Generate comprehensive test reports.\"\"\"\n        try:\n            # Generate HTML report\n            await self._generate_html_report(result)\n            \n            # Generate JSON report\n            await self._generate_json_report(result)\n            \n            # Generate trend analysis if we have history\n            if len(self.execution_history) > 1:\n                await self._generate_trend_report()\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate test reports: {e}\")\n    \n    async def _generate_html_report(self, result: TestResult) -> None:\n        \"\"\"Generate HTML test report.\"\"\"\n        html_template = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>API Gateway Test Report - {{ result.suite_name }}</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 20px; }\n        .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; }\n        .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }\n        .metric { background-color: #e9f5ff; padding: 15px; border-radius: 5px; text-align: center; }\n        .passed { background-color: #d4edda; }\n        .failed { background-color: #f8d7da; }\n        .test-details { margin: 20px 0; }\n        .test-item { border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }\n        .status-passed { color: #28a745; font-weight: bold; }\n        .status-failed { color: #dc3545; font-weight: bold; }\n        .status-error { color: #ffc107; font-weight: bold; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>API Gateway Test Report</h1>\n        <h2>{{ result.suite_name }}</h2>\n        <p>Execution ID: {{ result.execution_id }}</p>\n        <p>Generated: {{ result.timestamp.strftime('%Y-%m-%d %H:%M:%S') }} UTC</p>\n    </div>\n    \n    <div class=\"summary\">\n        <div class=\"metric{% if result.passed_tests == result.total_tests %} passed{% endif %}\">\n            <h3>Total Tests</h3>\n            <p>{{ result.total_tests }}</p>\n        </div>\n        <div class=\"metric passed\">\n            <h3>Passed</h3>\n            <p>{{ result.passed_tests }}</p>\n        </div>\n        <div class=\"metric{% if result.failed_tests > 0 %} failed{% endif %}\">\n            <h3>Failed</h3>\n            <p>{{ result.failed_tests }}</p>\n        </div>\n        <div class=\"metric\">\n            <h3>Success Rate</h3>\n            <p>{{ \"%.1f\" | format(result.success_rate) }}%</p>\n        </div>\n        <div class=\"metric\">\n            <h3>Execution Time</h3>\n            <p>{{ \"%.1f\" | format(result.execution_time_seconds) }}s</p>\n        </div>\n    </div>\n    \n    {% if result.performance_metrics %}\n    <div class=\"performance-metrics\">\n        <h3>Performance Metrics</h3>\n        <div class=\"summary\">\n            <div class=\"metric\">\n                <h4>Total Requests</h4>\n                <p>{{ result.performance_metrics.total_requests }}</p>\n            </div>\n            <div class=\"metric\">\n                <h4>Avg Response Time</h4>\n                <p>{{ \"%.3f\" | format(result.performance_metrics.average_response_time) }}s</p>\n            </div>\n            <div class=\"metric\">\n                <h4>Avg Throughput</h4>\n                <p>{{ \"%.1f\" | format(result.performance_metrics.average_throughput) }} RPS</p>\n            </div>\n        </div>\n    </div>\n    {% endif %}\n    \n    <div class=\"test-details\">\n        <h3>Test Details</h3>\n        {% for execution in result.test_executions %}\n        <div class=\"test-item\">\n            <h4>{{ execution.test_id }}</h4>\n            <p>Status: <span class=\"status-{{ execution.status.value }}\">{{ execution.status.value.upper() }}</span></p>\n            <p>Duration: {{ \"%.2f\" | format(execution.duration_seconds) }}s</p>\n            {% if execution.error_message %}\n            <p>Error: {{ execution.error_message }}</p>\n            {% endif %}\n        </div>\n        {% endfor %}\n    </div>\n</body>\n</html>\n        \"\"\"\n        \n        template = Template(html_template)\n        html_content = template.render(result=result)\n        \n        report_file = os.path.join(self.work_dir, 'reports', f'test_report_{result.execution_id}.html')\n        os.makedirs(os.path.dirname(report_file), exist_ok=True)\n        \n        with open(report_file, 'w') as f:\n            f.write(html_content)\n        \n        self.logger.info(f\"HTML report generated: {report_file}\")\n    \n    async def _generate_json_report(self, result: TestResult) -> None:\n        \"\"\"Generate JSON test report.\"\"\"\n        report_file = os.path.join(self.work_dir, 'reports', f'test_report_{result.execution_id}.json')\n        os.makedirs(os.path.dirname(report_file), exist_ok=True)\n        \n        with open(report_file, 'w') as f:\n            json.dump(asdict(result), f, indent=2, default=str)\n        \n        self.logger.info(f\"JSON report generated: {report_file}\")\n    \n    async def _generate_trend_report(self) -> None:\n        \"\"\"Generate trend analysis report.\"\"\"\n        try:\n            # Analyze trends over time\n            all_results = self.execution_history + self.execution_results\n            \n            # Group by suite name\n            suite_trends = {}\n            for result in all_results:\n                if result.suite_name not in suite_trends:\n                    suite_trends[result.suite_name] = []\n                suite_trends[result.suite_name].append(result)\n            \n            # Generate trend data\n            trend_data = {}\n            for suite_name, results in suite_trends.items():\n                # Sort by timestamp\n                results.sort(key=lambda x: x.timestamp)\n                \n                trend_data[suite_name] = {\n                    'success_rates': [r.success_rate for r in results],\n                    'execution_times': [r.execution_time_seconds for r in results],\n                    'timestamps': [r.timestamp.isoformat() for r in results],\n                    'total_tests': [r.total_tests for r in results]\n                }\n            \n            # Save trend report\n            trend_file = os.path.join(self.work_dir, 'reports', 'trend_analysis.json')\n            with open(trend_file, 'w') as f:\n                json.dump(trend_data, f, indent=2, default=str)\n            \n            self.logger.info(f\"Trend analysis report generated: {trend_file}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate trend report: {e}\")\n    \n    def get_test_status(self, test_id: str) -> Optional[TestExecution]:\n        \"\"\"Get current status of a specific test.\"\"\"\n        return self.active_executions.get(test_id)\n    \n    def list_available_suites(self) -> List[str]:\n        \"\"\"List all available test suites.\"\"\"\n        return list(self.suite_configurations.keys())\n    \n    def get_suite_configuration(self, suite_name: str) -> Optional[TestSuiteConfiguration]:\n        \"\"\"Get configuration for a specific test suite.\"\"\"\n        return self.suite_configurations.get(suite_name)\n    \n    def get_execution_history(self, suite_name: Optional[str] = None) -> List[TestResult]:\n        \"\"\"Get execution history, optionally filtered by suite name.\"\"\"\n        if suite_name:\n            return [r for r in self.execution_history if r.suite_name == suite_name]\n        return self.execution_history\n    \n    async def cleanup(self) -> None:\n        \"\"\"Clean up orchestrator resources.\"\"\"\n        # Cancel any running tests\n        for execution in self.active_executions.values():\n            # Mark as cancelled (would need proper task cancellation in real implementation)\n            execution.status = TestStatus.ERROR\n            execution.error_message = \"Test cancelled during cleanup\"\n        \n        # Close worker pool\n        if self.worker_pool:\n            self.worker_pool.shutdown(wait=True)\n        \n        # Close test components\n        if self.performance_benchmark:\n            await self.performance_benchmark.close()\n        if self.security_suite:\n            await self.security_suite.close()\n        \n        self.logger.info(\"Test orchestrator cleanup completed\")\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    async def main():\n        from auth.oauth_client import OAuthConfig, Platform\n        \n        # Create gateway configuration\n        oauth_config = OAuthConfig(\n            platform=Platform.ADVANA,\n            client_id=\"orchestrator-test-client\",\n            client_secret=\"orchestrator-test-secret\",\n            authorization_url=\"https://test-auth.mil/oauth/authorize\",\n            token_url=\"https://test-auth.mil/oauth/token\",\n            redirect_uri=\"https://localhost:8080/callback\",\n            scopes=[\"read\", \"write\", \"admin\"]\n        )\n        \n        gateway_config = DoDAGWConfig(\n            environment=APIGatewayEnvironment.DEVELOPMENT,\n            gateway_url=\"https://orchestrator-test-gateway.mil\",\n            client_certificate_path=\"/tmp/orchestrator-test-client.crt\",\n            private_key_path=\"/tmp/orchestrator-test-client.key\",\n            ca_bundle_path=\"/tmp/orchestrator-test-ca.crt\",\n            oauth_config=oauth_config,\n            service_name=\"orchestrator-test-service\",\n            service_version=\"1.0.0\",\n            security_classification=SecurityClassification.UNCLASSIFIED\n        )\n        \n        # Initialize test orchestrator\n        orchestrator = TestOrchestrator(gateway_config)\n        await orchestrator.initialize()\n        \n        try:\n            # List available test suites\n            print(\"Available test suites:\")\n            for suite in orchestrator.list_available_suites():\n                print(f\"- {suite}\")\n            \n            # Run smoke test suite\n            print(\"\\nRunning smoke test suite...\")\n            smoke_result = await orchestrator.execute_test_suite(\"smoke\")\n            print(f\"Smoke tests: {smoke_result.passed_tests}/{smoke_result.total_tests} passed ({smoke_result.success_rate:.1f}%)\")\n            \n            # Run security test suite\n            print(\"\\nRunning security test suite...\")\n            security_result = await orchestrator.execute_test_suite(\"security\")\n            print(f\"Security tests: {security_result.passed_tests}/{security_result.total_tests} passed ({security_result.success_rate:.1f}%)\")\n            \n            # Show execution history\n            history = orchestrator.get_execution_history()\n            print(f\"\\nTotal test executions in history: {len(history)}\")\n            \n        finally:\n            await orchestrator.cleanup()\n    \n    asyncio.run(main())